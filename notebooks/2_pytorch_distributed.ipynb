{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Overwriting ../models/model_2.py\n"
    }
   ],
   "source": [
    "%%writefile ../models/model_2.py\n",
    "import numpy as np\n",
    "import nlp\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Config, GPT2Model\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "\n",
    "# NEW\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "# NEW\n",
    "def init_process(rank, size, backend='gloo'):\n",
    "    \"\"\" Initialize the distributed environment. \"\"\"\n",
    "    os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "    os.environ['MASTER_PORT'] = '29500'\n",
    "    dist.init_process_group(backend, rank=rank, world_size=size)\n",
    "\n",
    "class IMDBDataset:\n",
    "    def __init__(self, part):\n",
    "        self.dataset = nlp.load_dataset('imdb')['train']\n",
    "        self.tokenizer = transformers.GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        review = self.dataset[idx]\n",
    "        label = torch.tensor(review['label'])\n",
    "        text = torch.tensor(self.tokenizer.encode(review['text']))\n",
    "        # The default GPT2 token length is 1024. The IMBD text review corpus is pretty long, and\n",
    "        # the GPT2 BPE tokenizer is pretty verbose, so we exceed this character limit in ~3% of\n",
    "        # cases. Since this is simple benchmark we are ignoring this problem (ConstantPad1d\n",
    "        # just clips the last few out words out).\n",
    "        text = nn.ConstantPad1d((1, 1024 - text.shape[0] - 1), 0)(text)\n",
    "        return {'text': text, 'label': label}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.dataset.num_rows\n",
    "\n",
    "\n",
    "class IMDBSentimentClassificationModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.gpt2_config = transformers.GPT2Config()\n",
    "        self.gpt2_model = transformers.GPT2Model(self.gpt2_config)\n",
    "        self.head = nn.Sequential(*[\n",
    "            nn.Linear(768, 2**6),\n",
    "            nn.Linear(2**6, 2**4),\n",
    "            nn.Linear(2**4, 2),\n",
    "            nn.LogSoftmax(dim=0)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        hidden_states, _ = self.gpt2_model(tokens)\n",
    "        final_hidden_state = hidden_states[:, -1, :]\n",
    "        out = self.head(final_hidden_state)\n",
    "        return out\n",
    "\n",
    "\n",
    "def get_dataloader(rank, world_size):\n",
    "    dataset = IMDBDataset('train')    \n",
    "    \n",
    "    # NEW\n",
    "    sampler = DistributedSampler(dataset, rank=rank, num_replicas=world_size, shuffle=True)\n",
    "    dataloader = DataLoader(dataset, batch_size=4, sampler=sampler)\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "def get_model():\n",
    "    return IMDBSentimentClassificationModel()\n",
    "\n",
    "def train(rank, num_epochs, world_size):\n",
    "    # NEW\n",
    "    init_process(rank, world_size)\n",
    "    print(f\"Rank {rank}/{world_size} training process initialized.\\n\")\n",
    "        \n",
    "    # NEW\n",
    "    # Since this is a single-instance multi-GPU training script, it's important\n",
    "    # that only one process handle downloading of the data, to avoid race conditions\n",
    "    # implicit in having multiple processes attempt to write to the same file\n",
    "    # simultaneously.\n",
    "    if rank == 0:\n",
    "        nlp.load_dataset('imdb')\n",
    "        transformers.GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    dist.barrier()\n",
    "    print(f\"Rank {rank}/{world_size} training process passed data download barrier.\\n\")\n",
    "    \n",
    "    model = get_model()\n",
    "    model.cuda(rank)\n",
    "    model.train()\n",
    "\n",
    "    # NEW\n",
    "    model = DistributedDataParallel(model, device_ids=[rank])\n",
    "    \n",
    "    dataloader = get_dataloader(rank, world_size)\n",
    "\n",
    "    loss_fn = nn.NLLLoss()\n",
    "    optimizer = Adam(model.parameters())\n",
    "    \n",
    "    writer = SummaryWriter(f'/spell/tensorboards/model_2')\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        losses = []\n",
    "\n",
    "        for idx, batch in enumerate(dataloader):\n",
    "            tokens, labels = batch['text'], batch['label']\n",
    "            tokens = tokens.cuda(rank)\n",
    "            labels = labels.cuda(rank)\n",
    "\n",
    "            model.zero_grad()\n",
    "            y_pred = model(tokens)\n",
    "            \n",
    "            loss = loss_fn(y_pred, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            if idx % 10 == 0:\n",
    "                print(\n",
    "                    f'Finished epoch {epoch}, rank {rank}/{world_size}, batch {idx}. '\n",
    "                    f'Loss: {loss:.3f}.\\n'\n",
    "                )\n",
    "            if rank == 0:\n",
    "                writer.add_scalar('training loss', loss)\n",
    "            losses.append(loss)\n",
    "\n",
    "        print(\n",
    "            f'Finished epoch {epoch}, rank {rank}/{world_size}. '\n",
    "            f'Avg Loss: {np.mean(losses)}; Median Loss: {np.min(losses)}.\\n'\n",
    "        )\n",
    "        \n",
    "        if rank == 0:\n",
    "            if not os.path.exists('/spell/checkpoints/'):\n",
    "                os.mkdir('/spell/checkpoints/')\n",
    "            torch.save(model.state_dict(), f'/spell/checkpoints/model_{epoch}.pth')\n",
    "\n",
    "# NEW\n",
    "NUM_EPOCHS = 20\n",
    "WORLD_SIZE = torch.cuda.device_count()\n",
    "def main():\n",
    "    mp.spawn(train,\n",
    "        args=(NUM_EPOCHS, WORLD_SIZE),\n",
    "        nprocs=WORLD_SIZE,\n",
    "        join=True)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 0/4 training process initialized.\n",
      "\n",
      "Rank 2/4 training process initialized.\n",
      "\n",
      "Rank 1/4 training process initialized.\n",
      "\n",
      "Rank 3/4 training process initialized.\n",
      "\n",
      "Rank 3/4 training process passed data download barrier.\n",
      "\n",
      "Rank 1/4 training process passed data download barrier.\n",
      "\n",
      "Rank 0/4 training process passed data download barrier.\n",
      "\n",
      "Rank 2/4 training process passed data download barrier.\n",
      "\n",
      "Finished epoch 1, rank 3/4, batch 0. Loss: 0.672.\n",
      "\n",
      "Finished epoch 1, rank 0/4, batch 0. Loss: 0.791.\n",
      "\n",
      "Finished epoch 1, rank 1/4, batch 0. Loss: 0.698.\n",
      "\n",
      "Finished epoch 1, rank 2/4, batch 0. Loss: 0.749.\n",
      "\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1132 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Finished epoch 1, rank 1/4, batch 10. Loss: 0.653.\n",
      "\n",
      "Finished epoch 1, rank 0/4, batch 10. Loss: 0.693.\n",
      "\n",
      "Finished epoch 1, rank 3/4, batch 10. Loss: 0.693.\n",
      "Finished epoch 1, rank 2/4, batch 10. Loss: 0.672.\n",
      "\n",
      "\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1236 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Finished epoch 1, rank 0/4, batch 20. Loss: 0.701.\n",
      "\n",
      "Finished epoch 1, rank 1/4, batch 20. Loss: 0.695.\n",
      "\n",
      "Finished epoch 1, rank 3/4, batch 20. Loss: 0.698.\n",
      "\n",
      "Finished epoch 1, rank 2/4, batch 20. Loss: 0.694.\n",
      "\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1128 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1158 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Finished epoch 1, rank 0/4, batch 30. Loss: 0.721.\n",
      "Finished epoch 1, rank 1/4, batch 30. Loss: 0.693.\n",
      "\n",
      "\n",
      "Finished epoch 1, rank 2/4, batch 30. Loss: 0.693.\n",
      "\n",
      "Finished epoch 1, rank 3/4, batch 30. Loss: 0.696.\n",
      "\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1243 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Finished epoch 1, rank 3/4, batch 40. Loss: 0.693.\n",
      "Finished epoch 1, rank 1/4, batch 40. Loss: 0.693.\n",
      "\n",
      "\n",
      "Finished epoch 1, rank 0/4, batch 40. Loss: 0.693.\n",
      "\n",
      "Finished epoch 1, rank 2/4, batch 40. Loss: 0.694.\n",
      "\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1047 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Finished epoch 1, rank 1/4, batch 50. Loss: 0.702.\n",
      "Finished epoch 1, rank 2/4, batch 50. Loss: 0.693.\n",
      "\n",
      "\n",
      "Finished epoch 1, rank 0/4, batch 50. Loss: 0.693.\n",
      "\n",
      "Finished epoch 1, rank 3/4, batch 50. Loss: 0.704.\n",
      "\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"../models/model_2.py\", line 154, in <module>\n",
      "    main()\n",
      "  File \"../models/model_2.py\", line 151, in main\n",
      "    join=True)\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/spawn.py\", line 171, in spawn\n",
      "    while not spawn_context.join():\n",
      "  File \"/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/spawn.py\", line 77, in join\n",
      "    timeout=timeout,\n",
      "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 920, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/usr/lib/python3.7/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python ../models/model_2.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}